---
title: "Human Resources Data Clustering & PC Analysis"
author: "Kar Ng"
date: '2022-05'
output: 
  github_document: 
    toc: true
    toc_depth: 4
always_allow_html: yes
---


## 1 SUMMARY



## 2 R PACKAGES 

```{r, warning=FALSE, message=FALSE}

library(tidyverse)
library(kableExtra)
library(lubridate)
library(skimr)
library(tidytext)
library(factoextra)
library(FactoMineR)
library(cluster)    # For daisy function
library(cowplot)
library(Rtsne)

```



## 3 INTRODUCTION

This project analyses a set of human resource data by applying one of the core machine learning technique, "Principal Components (PC) methods", which belong to the "unsupervised" branch of machine learning domain. 

There are 5 types of principal components methods:

* Principal Component Analysis (PCA)    
* Correspondence Analysis (CA)    
* Multiple Correspondence Analysis (MCA)  
* Factor Analysis of Mixed Data (FAMD)  
* Multiple Factor Analysis (MFA)  

These PC methods are designed for different type of datasets, I would not explain why and how are these methods different from each other here and for which type of datasets but I will apply the most appropriate one for the dataset used in this project.

Principal component methods are typically used for multivariate analysis (MVA) when we are analysing datasets that have many variables. PC methods will quickly help us identifying the most important variables that contribute the most in explaining the variations in the data sets. 

During computation, PC methods will extract all the variations in the multivariate dataframe and expressed them into a few new variables called principal components (there are other inter-changeable terms with similar meaning in this context, such as "dims" or "axes"). After previous step, many special plots of PC will be plotted to understand the result. Important to note that the goal of PC methods is to identify main directions along which the variation is maximal (KASSAMBARA A 2017). 


## 4 DATA PREPARATION

A public dataset called "Human Resources Data Set" by DR.RICH on Kaggle.com has been downloaded for analysis. *Kaggle.com* is a popular website for data science community to share datasets, codes and knowledge. 

This dataset 

### 4.1 Data import

Following code upload the dataset into R.

```{r}
hr <- read.csv("hr_dataset.csv", 
               fileEncoding = "UTF-8-BOM",
               na.strings = T,
               header = T,
               row.names = NULL)

```



### 4.2 Data description

Following is the data dictionary/description of this dataset, downloaded from this link: [Rpubs](https://rpubs.com/rhuebner/hrd_cb_v14), created by the author, Dr. Rich Huebner.

![Data description](https://raw.githubusercontent.com/KAR-NG/hr/main/pic1_data_description.png)

### 4.3 Data exploration

There are 311 rows and 35 columns in the dataset. Following show the variable "type" allocated by R to each of the column (also known as variables or features), along with several starting values of these variables.

```{r}
glimpse(hr)

```
Randomly sample 10 rows of data from the table:

```{r}
sample_n(hr, 10)

```

The first column is a column recording employee names. I have made this column the name of each rows (or known as observation). It is the standard format required for PC methods.  

## 5 DATA CLEANING

The data may seem perfectly to go however numerous important cleaning and manipulation tasks have been identified and will be perfectly completed in this section. 

```{r}
hr <- hr %>% 
  column_to_rownames(var = "Employee_Name")

```




### 5.1 Variables removals

I will be removing some irrelevant or repeated variables that may not help in our analysis to understand the hidden trends in the data. They are:

* EmpID  
* MaritalStatusID  
* GenderID  
* EmpStatusID  
* DeptID  
* PerfScoreID  
* PositionID  
* Zip  
* ManagerID  
* LastPerformanceReview_Date

After removal of above features, the numbers of columns have been reduced from 35 to 25. 

```{r}
hr2 <- hr %>%
  dplyr::select(-EmpID, -MaritalStatusID, -GenderID, -EmpStatusID, -DeptID, -PerfScoreID, -PositionID, -Zip, -ManagerID, -LastPerformanceReview_Date)
  
glimpse(hr2)

```
### 5.2 New Variable: Age

At the year of writing this project is 2022, and therefore the calculation of age will be 2022 minus DOB (date of birth) in the dataset. The DOB will be replaced with "Age".


```{r}
hr2 <- hr2 %>%  
  mutate(yearDOB = substr(DOB, start = 7, stop = 8),
         yearbirth = as.numeric(paste0(19, yearDOB)),
         Age = 2022 - yearbirth) %>%   
  relocate(Age, .after = State) %>% 
  dplyr::select(-DOB, -yearDOB, -yearbirth)

```


Now, the variable "DOB" has been replaced by "Age", 7th variable, and following shows the age of all employees in the dataset. 

```{r}
hr2$Age
```

### 5.3 New Variable: years_worked

There are two variables in the date set: DateofHire and DateofTermination. 

I will compute the days of each employee worked/works by using the date of termination (DateofTermination) minus date of hire (DateofHire), and for present employee I will use today's date (5-May-2022) minus the date of hire to obtain the total number of days worked. 

```{r}

hr2 <- hr2 %>% 
  mutate(DateofHire = mdy(DateofHire),
         DateofTermination = mdy(DateofTermination),
         days_worked = ifelse(is.na(DateofTermination),
                              today() - DateofHire,
                              DateofTermination - DateofHire),
         years_worked = round(days_worked/365, 1)) %>% 
  relocate(years_worked, .after = RaceDesc) %>% 
  dplyr::select(-DateofHire, -DateofTermination, -days_worked)

```

Following shows number of years, with 1 decimal place, worked by each employee in the dataset. 

```{r}
hr2$years_worked

```

### 5.4 Trim

This section trim the unnecessary leading and trailing whitespaces of character variables in the dataset. 

```{r}
hr2 <- hr2 %>% 
  mutate_if(is.character, trimws)

```



### 5.5 Factor conversion

Some "numeric" and "textual" features will need to be converted into "factor" type because of their categorical nature. 

* All textual features that formed by character "chr" in the dataset need conversion. Following shows all of these textual variables in the datasets.       

```{r}
str(hr2 %>% 
  select(is.character))

```
Following codes complete the conversion task for these textual features. 

```{r}
hr2 <- hr2 %>% 
  mutate_if(is.character, as.factor)

```

* With regards to numeric features, features that need to be converted into factor type are MarriedID, FromDiversityJobFairID, Termd, and EmpSatisfaction.

```{r}
str(hr2 %>% 
      select(-is.factor))

```
Following codes complete the conversion task for these selected numerical features.  

```{r}
hr2 <- hr2 %>% 
  mutate(MarriedID = as.factor(MarriedID),
         FromDiversityJobFairID = as.factor(FromDiversityJobFairID),
         Termd = as.factor(Termd),
         EmpSatisfaction = as.factor(EmpSatisfaction))

```

After conversion, we are able to use following code to summaries the dataset. For example, there are 187 of "0" and 124 of "1" for Married ID. The type of this variable has to be in factor form to make this summary feasible. 

Different categories (or known as "level") in each factor variables are now feasible and countable.

```{r}
summary(hr2 %>% 
          dplyr::select(is.factor))

```

### 5.6 CitizenDesc

There are three categories for the variable "CitizenDesc", Eligible NonCitizen (12 employees), Non-Citizen (4 employees) and 295 US Citizen employees. I can't see why I can't merge "Eligible NonCitizen" and "Non-Citizen", and therefore this section will perform this task. 12 of the "Eligible NonCitizen" will be grouped to "Non-Citizen". 

```{r}
hr2 <- hr2 %>% 
  mutate(CitizenDesc = fct_recode(CitizenDesc,
                                  "Non-Citizen" = "Eligible NonCitizen"))

```

Let's check, and the conversion has been completed. 

```{r}
table(hr2$CitizenDesc)

```


### 5.7 HispanicLatino

The HispanicLatino has following 4 categories. 

```{r}
table(hr2$HispanicLatino)
```
The "no" and "Yes" should be a mistake and have to be converted to "No" and "Yes". Following code complete the conversion. 

```{r}
hr2 <- hr2 %>% 
  mutate(HispanicLatino = fct_recode(HispanicLatino,
                                  "No" = "no",
                                  "Yes" = "yes"))
```


Let's check, and the conversion has been completed. 

```{r}
table(hr2$HispanicLatino)

```

### 5.8 Missing data check

This section check missing data ("NA") in the dataset and will be managed accordingly. 

```{r}
skim_without_charts(hr2)
```
From the above function, there is no any missing data detected by the column "n_missing" or by the column "complete_rate".

Alternatively, I can count the number of missing value ("NA") in each column by following code. 

```{r}
colSums(is.na(hr2))

```

There is no any missing data detected.


## 6 VISUALISATION

This section will help to understand data distribution of each variable and more importantly, to detect outliers.

There are different type of outliers, some outliers may arise from typos but some may be real outliers. For example, executive members may have significant higher salary than most of the employees. This section will find and deal with false outliers, which are those that may be resulted from human errors. 
The true outliers would not be an issue in this project, because principal components methods will scale all numerical variables to make all variables comparable, and therefore this step, known as standardisation, will transform these true outliers. 

### 6.1 Numerical variables

Insights from following summary:

* All employees have their days of absences for worked evenly distributed between 0 to 20  
* Most employees have age between 32 to 55  
* Employees were very on time to work but except a few  
* Employees were very engaging with majority of the scores fall between 4 to 5  
* Typical salary range between 50k to 70k approximately  
* There are a small number of employees had many special projects  
* Most employees from this company worked between 5 to 10 years  

```{r, message=FALSE}
# df

df6.1 <- hr2 %>% 
  dplyr::select(-is.factor) %>% 
  pivot_longer(1:7, names_to = "my_var", values_to = "my_values")

# graph

ggplot(df6.1, aes(x = my_values, fill = my_var)) +
  geom_histogram(color = "black") +
  facet_wrap(~my_var, scales = "free") +
  theme_bw() +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  labs(title = "Visualisation of Numerical variables",
       subtitle = "by Histogram",
       x = "Variables",
       y = "Count")


```

### 6.2 factor variables 1

There are 17 factor variables to look at, I have spread this examination into two parts. In this part "factor variables 1", I will look at the first 9 factor variables and examine the remaining in next section. 

```{r, fig.height=9, fig.width=10, message=FALSE}
# df

df6.2 <- hr2 %>% 
  dplyr::select(is.factor) %>% 
  dplyr::select(-Position) %>% 
  pivot_longer(1:8, names_to = "my_var", values_to = "my_values") %>% 
  group_by(my_var, my_values) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(label = reorder_within(x = my_values, by = count, within = my_var))

# graph

ggplot(df6.2, aes(y = label, x = count, fill = my_values)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = count), hjust = 1) +
  facet_wrap(~my_var, scales = "free", ) +
  theme_bw() +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  scale_y_reordered() +
  labs(title = "Visualisation of factor variables 1",
       subtitle = "by Bar chart",
       y = "Variables",
       x = "Count")


```

Following shows the number of workers in each position in the organisation.

```{r, fig.width=8, fig.height=6}
hr2 %>% 
  dplyr::select(Position) %>% 
  group_by(Position) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(y = fct_reorder(Position, count), x = count)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = count), hjust = -0.3) +
  theme_bw() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  labs(title = "Visualisation of factor variables 1",
       subtitle = "by Bar chart",
       y = "Variables",
       x = "Count") +
  scale_x_continuous(lim = c(0, 140))
  

```


### 6.3 factor variables 2

This section examine the remaining 8 factor variables. 

```{r, fig.height=9, fig.width=10, message=FALSE}
# df

df6.3 <- hr2 %>% 
  dplyr::select(is.factor) %>% 
  pivot_longer(10:17, names_to = "my_var", values_to = "my_values") %>% 
  group_by(my_var, my_values) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(label = reorder_within(x = my_values, by = count, within = my_var))

# graph

ggplot(df6.3, aes(y = label, x = count, fill = my_values)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = count), hjust = 1) +
  facet_wrap(~my_var, scales = "free", ) +
  theme_bw() +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  scale_y_reordered() +
  labs(title = "Visualisation of factor variables 1",
       subtitle = "by Bar chart",
       y = "Variables",
       x = "Count")


```

### 6.4 Distribution study of continuous variable

A number of continuous variables are non-normality and contains outliers. 

```{r}
# set df

df6.4 <- hr2 %>% select_if(is.numeric)
df6.4 <- df6.4 %>% 
  pivot_longer(c(1:7), names_to = "myvar", values_to = "myval") %>% 
  mutate(myvar = as.factor(myvar))

# plot

ggplot(df6.4, aes(y = myvar, x = myval)) +
  geom_boxplot() +
  facet_wrap(~myvar, scales = "free")

```


After completing basic simple visualisation, we will now dive into more advanced data mining technique. 

## 7 CLUSTERING

Clustering is a series of different techniques to find distinct groups of data within a dataset. To be specific in terms of "observation" and "variable", clustering is trying to group similar observations together, and each group should be distinct from each other. 

### 7.1 Distance metrics

The data is a mixed dataset and a special type of distance metrics to measure distance between observation will be used which is called Gower distance. For continuous variables inside the dataset, the Gower function will use manhattan distance, whereas for categorical variables, Gower function will use dice distance. Gower will then combine all these distance together and form a single distance value and which can be known as Gower distance. 

To compute gower distance between observations,

```{r}
glimpse(hr2)

```


```{r}
gower.dis <- daisy(hr2, 
                   metric = "gower",
                   type = list(logratio = c("Age", "DaysLateLast30", "Salary", "SpecialProjectsCount"))) # log transformation to which column?

summary(gower.dis)

```
The Gower distance metric has now been computed for 311 rows of observations for this dataset with mixed data-types. 

Following show the most similar and different pair in the dataset. 

```{r}
gower_mat <- as.matrix(gower.dis)

hr2[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]),   # min for most similar
          arr.ind = T)[1, ], ]

```

```{r}
hr2[which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]),   # max for most different
          arr.ind = T)[1, ], ]

```

### 7.1 Gower with PAM

K-means clustering method cannot be applied on the daisy function, and therefore the clustering algorithms typically used for Gower distance is Partitioning around Medoid (PAM) and Hierarchical clustering. 

**Determining Best K**

This section will perform PAM, however, the optimal will need to be determined. I will use silhouette width for this task, which is one of the internal cluster validation to see the quality of cluster. It has value ranges from -1 to 1, the higher the silhouette metric, the better the clustering. In fact, the overall silhouette metric of a k (number of clustering) is computed from all silhouette metric of individual observation within relevant cluster.  

Applying for-loop for all silhouette width of each K:

```{r}

sil_df <- c(NA)

for (i in 2:10) {
  res.pam <- pam(gower.dis, diss = T, k = i)
  sil_df[i] <- res.pam$silinfo$avg.width
}

sil_df

```
Plot the graph:

```{R}

plot(sil_df,
     xlab = "Number of Cluster (K)",
     ylab = "Silhouette Width",
     bty = "n")
lines(sil_df)

```
**PAM for K = 2**

Silhouette plot suggests that the dataset is best to be clustered into 2 groups. Therefore, I will create a PAM to cluster the dataset into 2 clusters (2 K).

```{r}
res.pam <-  pam(gower.dis, diss = T, k = 2)

```

All observations in the dataset have been clustered into 2 clusters and each cluster has following size.  

```{r}
table(res.pam$clustering)

```
**Add Cluster groups to Data**

In this step, I add the clustering result into the dataset for further analysis. 

```{r}

hr2_pam <- cbind(hr2, cluster = res.pam$clustering) %>% 
  mutate(cluster = as.factor(cluster)) %>% 
  relocate(cluster, .before = MarriedID)

```

**Analyse the result of PAM clustering**

Following is the summary of variables from cluster 1:

```{r}
cluster1_stat <- hr2_pam %>% 
  filter(cluster == "1") 
  
summary(cluster1_stat[, -1]) 

```
Following is the summary of variables from cluster 2:

```{r}
cluster2_stat <- hr2_pam %>% 
  filter(cluster == "2") 
  
summary(cluster2_stat[, -1])

```

Following is the two medoids used to form the two clusters, and which might be helpful for understanding and interpreting result. 

```{r}
hr2[res.pam$medoids, ]

```

**Visualisation with t-SNE**

```{r}
# Make a tsne object with gower dis of the dataset

tsne_object <- Rtsne(gower.dis, is_distance = T)

# tsne df

tsne_df <- tsne_object$Y %>% 
  data.frame() %>% 
  rename("X" = "X1",
         "Y" = "X2") %>%     # Extract XY of tsne object
  mutate(cluster = res.pam$clustering,
         cluster = as.factor(cluster))
  
# Plot

ggplot(tsne_df, aes(x = X, y = Y, color = cluster)) +
  geom_point(alpha = 0.5, size = 3)  +
  theme_classic() +
  scale_color_brewer(palette = "Set1")


```
The plot shows two separated clusters that PAM was able to detect. These two clusters are not perfect with several contamination, but these are minority.   




## 8 PRINCIPAL COMPONENT

Following code saves the cleaned dataset into csv into the allocated file for storage purpose.

```{r}
write.csv(hr2_pam, "hr2_pam.csv")
```

Import the csv file back and a little transformation (convert character variable into factor.)

```{r}

hr_data <- read.csv("hr2_pam.csv", row.names = 1)
hr_data <- hr_data %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(cluster = as.factor(cluster)
         ) 

```

```{r}
kar.famd <- FAMD(hr_data, graph = F)
```

```{r}
fviz_famd_ind(kar.famd, 
              habillage = "cluster",
              repel = T)

```









## REFERENCE

Clustering and dimensionality reduction techniques on the Berlin Airbnb data and the problem of mixed data (n.d.),viewed 15 May 2022 https://rstudio-pubs-static.s3.amazonaws.com/579984_6b9efbf84ee24f00985c29e24265d2ba.html

KASSAMBARA A 2017, *Practical Guide To Principal Component Methods in R*, Edition 1, sthda.com

Rich Huebner 2020, *Human Resources Data Set*, viewed 2 May 2022, https://www.kaggle.com/datasets/rhuebner/human-resources-data-set?resource=download 

Rich Huebner 2021, *Codebook - HR Dataset v14*, viewed 3 May 2022, https://rpubs.com/rhuebner/hrd_cb_v14

Wicked Good Data - r 2016, *https://www.r-bloggers.com/2016/06/clustering-mixed-data-types-in-r/*, viewed 8 May 2022